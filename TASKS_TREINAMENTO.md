# TASKS_TREINAMENTO.md â€” Plano de Treinamento do Assessor.AI

> **Objetivo:** Tornar o agente altamente preciso na geraÃ§Ã£o de minutas de admissibilidade, usando minutas de referÃªncia, feedback humano e avaliaÃ§Ã£o contÃ­nua.
>
> **Resultado esperado:** Minutas geradas com excelÃªncia jurÃ­dica, formato 100% consistente, zero alucinaÃ§Ãµes, e economia de tokens.

---

## VisÃ£o Geral da Arquitetura de Treinamento

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SISTEMA DE TREINAMENTO                       â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Base de       â”‚   â”‚ Seletor de   â”‚   â”‚ InjeÃ§Ã£o no Prompt  â”‚  â”‚
â”‚  â”‚ Minutas Gold  â”‚â”€â”€â–¶â”‚ Similaridade â”‚â”€â”€â–¶â”‚ (few-shot Etapa 3) â”‚  â”‚
â”‚  â”‚ (aprovadas)   â”‚   â”‚ (RAG leve)   â”‚   â”‚                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Feedback      â”‚   â”‚ Auto-Eval    â”‚   â”‚ RegressÃ£o          â”‚  â”‚
â”‚  â”‚ do Assessor   â”‚â”€â”€â–¶â”‚ por Rubrica  â”‚â”€â”€â–¶â”‚ ContÃ­nua           â”‚  â”‚
â”‚  â”‚ (aceitar/rej) â”‚   â”‚              â”‚   â”‚                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Sprint 1 â€” Base de Minutas de ReferÃªncia (Gold Standard)

> **Meta:** Criar a infraestrutura para armazenar, indexar e recuperar minutas aprovadas.

### Tarefa 1.1 â€” Estrutura de DiretÃ³rios e Formato

- [ ] Criar diretÃ³rio `minutas_referencia/` na raiz do projeto
- [ ] Criar subdiretÃ³rios por tipo de recurso:
  ```
  minutas_referencia/
  â”œâ”€â”€ recurso_especial/
  â”‚   â”œâ”€â”€ admitido/
  â”‚   â””â”€â”€ inadmitido/
  â”œâ”€â”€ recurso_extraordinario/
  â”‚   â”œâ”€â”€ admitido/
  â”‚   â””â”€â”€ inadmitido/
  â””â”€â”€ metadata.json
  ```
- [ ] Definir formato padrÃ£o para cada minuta de referÃªncia: arquivo `.json` contendo:
  ```json
  {
    "id": "gold_001",
    "tipo_recurso": "recurso_especial",
    "decisao": "inadmitido",
    "materias": ["reexame de prova", "sÃºmula 7/STJ"],
    "sumulas_aplicadas": ["7/STJ", "283/STF"],
    "tags": ["civel", "responsabilidade_civil", "dpvat"],
    "etapa1_resumo": "Recurso Especial com base no art. 105, III, 'a', CF...",
    "etapa2_resumo": "O acÃ³rdÃ£o fundamentou em reexame de matÃ©ria fÃ¡tica...",
    "minuta_completa": "RECURSO ESPECIAL CÃVEL NÂº ...",
    "avaliacao_humana": "aprovada",
    "assessor_revisor": "nome_do_assessor",
    "data_aprovacao": "2026-02-19",
    "notas_revisao": "Minuta correta, sem ajustes necessÃ¡rios"
  }
  ```

### Tarefa 1.2 â€” Script de ImportaÃ§Ã£o de Minutas

- [ ] Criar `scripts/importar_minuta.py` para facilitar o cadastro:
  - Recebe o texto da minuta (Markdown ou texto puro)
  - Extrai automaticamente: tipo de recurso, decisÃ£o, sÃºmulas mencionadas
  - Gera tags a partir das matÃ©rias controvertidas
  - Salva no formato JSON padrÃ£o em `minutas_referencia/`
  - Cria o embedding de texto para busca por similaridade (Tarefa 2.1)
- [ ] Suportar importaÃ§Ã£o em lote de um diretÃ³rio com minutas `.md` ou `.docx`

### Tarefa 1.3 â€” Curadoria Inicial (10-20 Minutas Gold)

- [ ] Solicitar ao usuÃ¡rio/assessor 10-20 minutas que considere exemplares
- [ ] Importar usando o script da Tarefa 1.2
- [ ] Garantir diversidade:
  - MÃ­nimo 5 inadmitidos (com diferentes sÃºmulas)
  - MÃ­nimo 3 admitidos
  - MÃ­nimo 2 admissÃ£o parcial
  - Cobrir: SÃºmulas 7, 211, 282, 283, 284, 126
- [ ] Validar formataÃ§Ã£o JSON e campos obrigatÃ³rios

---

## Sprint 2 â€” Seletor de Similaridade (RAG Leve)

> **Meta:** Dado um caso novo, encontrar a minuta de referÃªncia mais parecida para usar como exemplo.

### Tarefa 2.1 â€” GeraÃ§Ã£o de Embeddings

- [ ] Criar `src/minuta_embeddings.py`:
  - Usar embeddings leves (e.g., `text-embedding-3-small` da OpenAI ou modelo local)
  - Gerar embedding para cada minuta gold baseado em:
    - Tipo de recurso + matÃ©rias controvertidas + sÃºmulas
  - Salvar embeddings em `minutas_referencia/embeddings.json`
- [ ] Custo-eficiente: gerar embeddings uma Ãºnica vez e cachear (nÃ£o a cada request)

### Tarefa 2.2 â€” Buscador de Minuta Similar

- [ ] Criar `src/minuta_selector.py` com funÃ§Ã£o:
  ```python
  def selecionar_minuta_referencia(
      tipo_recurso: str,
      materias: list[str],
      sumulas: list[str],
      top_k: int = 1
  ) -> dict | None:
  ```
- [ ] CritÃ©rios de seleÃ§Ã£o (por prioridade):
  1. **Mesmo tipo de recurso** (especial vs extraordinÃ¡rio) â€” eliminatÃ³rio
  2. **Mesma decisÃ£o estimada** (admitido/inadmitido) â€” peso 3x
  3. **SÃºmulas em comum** â€” peso 2x
  4. **MatÃ©rias similares** â€” peso 1x via similaridade de embedding
- [ ] Fallback: se nenhuma minuta tiver similaridade > 0.5, retornar `None` (nÃ£o forÃ§ar exemplo ruim)

### Tarefa 2.3 â€” Cache e Performance

- [ ] PrÃ©-carregar embeddings na inicializaÃ§Ã£o do app (nÃ£o a cada request)
- [ ] Manter Ã­ndice em memÃ³ria (sÃ£o apenas 10-20 minutas, nÃ£o precisa de banco vetorial)
- [ ] Tempo mÃ¡ximo de seleÃ§Ã£o: <100ms

---

## Sprint 3 â€” InjeÃ§Ã£o no Prompt da Etapa 3

> **Meta:** Usar a minuta selecionada como exemplo (few-shot) no prompt da Etapa 3, sem ultrapassar o contexto.

### Tarefa 3.1 â€” Modificar Prompt da Etapa 3

- [ ] Editar `prompts/dev_etapa3.md` adicionando seÃ§Ã£o de referÃªncia:
  ```markdown
  ## Minuta de ReferÃªncia (Exemplo)

  A minuta abaixo Ã© um exemplo aprovado de caso similar. Use-a como
  referÃªncia de FORMATO, ESTILO e LINGUAGEM. NÃƒO copie o conteÃºdo â€”
  adapte para os fatos e fundamentos do caso atual.

  ---
  {minuta_referencia}
  ---
  ```
- [ ] InstruÃ§Ã£o explÃ­cita no prompt:
  - "Siga o formato e estilo da minuta de referÃªncia"
  - "NÃƒO copie dados/fatos â€” extraia exclusivamente das Etapas 1 e 2"
  - "A minuta de referÃªncia serve APENAS como modelo de escrita e estrutura"

### Tarefa 3.2 â€” Integrar no Pipeline

- [ ] Modificar `src/etapa3.py` para:
  1. ApÃ³s Etapa 2, extrair: tipo de recurso, matÃ©rias, sÃºmulas
  2. Chamar `selecionar_minuta_referencia()` com esses dados
  3. Se encontrar minuta similar, injetar no prompt da Etapa 3
  4. Se nÃ£o encontrar (None), prosseguir sem exemplo (comportamento atual)
- [ ] Modificar `src/prompt_loader.py` para suportar variÃ¡vel `{minuta_referencia}` no template

### Tarefa 3.3 â€” Controle de Tokens

- [ ] Limitar a minuta de referÃªncia a **mÃ¡ximo 3.000 tokens** no prompt
- [ ] Se a minuta ultrapassar, truncar pela seÃ§Ã£o III (manter I e II como exemplo)
- [ ] Logar no metadata: `"minuta_referencia_usada": "gold_007"` ou `null`
- [ ] Adicionar ao `.env`: `ENABLE_MINUTA_REFERENCIA=true` (feature flag)

---

## Sprint 4 â€” Feedback do Assessor (Loop de Treinamento)

> **Meta:** Permitir que o assessor avalie cada minuta gerada e alimentar isso de volta no sistema.

### Tarefa 4.1 â€” UI de Feedback na Interface Web

- [ ] ApÃ³s exibiÃ§Ã£o do resultado, adicionar botÃµes de avaliaÃ§Ã£o:
  ```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Como avalia esta minuta?            â”‚
  â”‚                                      â”‚
  â”‚  [âœ… Aprovada]  [âš ï¸ Parcial]  [âŒ Reprovada] â”‚
  â”‚                                      â”‚
  â”‚  ComentÃ¡rios (opcional):             â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
  â”‚  â”‚                                â”‚  â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
  â”‚                                      â”‚
  â”‚  [ğŸ’¾ Salvar como Minuta ReferÃªncia]  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  ```
- [ ] Campos:
  - `avaliacao`: aprovada | parcial | reprovada
  - `comentarios`: texto livre (o que estava errado, o que melhorar)
  - `salvar_como_gold`: checkbox (se aprovada, adicionar Ã  base de referÃªncia)

### Tarefa 4.2 â€” API de Feedback

- [ ] Criar endpoint `POST /feedback` no `web_app.py`:
  ```python
  @app.post("/feedback/<job_id>")
  def feedback(job_id: str):
      avaliacao = request.form["avaliacao"]
      comentarios = request.form.get("comentarios", "")
      salvar_gold = request.form.get("salvar_gold") == "true"
      # Salvar feedback em outputs/feedback/
      # Se salvar_gold, importar automaticamente para minutas_referencia/
  ```
- [ ] Salvar feedback em `outputs/feedback/{timestamp}_{job_id}.json`

### Tarefa 4.3 â€” Auto-PromoÃ§Ã£o para Gold Standard

- [ ] Quando assessor marca "Aprovada" + "Salvar como ReferÃªncia":
  1. Extrair tipo de recurso, matÃ©rias, sÃºmulas da anÃ¡lise
  2. Gerar JSON no formato padrÃ£o (Tarefa 1.1)
  3. Gerar embedding (Tarefa 2.1)
  4. Adicionar a `minutas_referencia/` automaticamente
- [ ] A base cresce organicamente com o uso â€” sem intervenÃ§Ã£o manual

---

## Sprint 5 â€” Auto-AvaliaÃ§Ã£o por Rubrica

> **Meta:** O prÃ³prio pipeline avalia a qualidade da minuta antes de entregar, usando critÃ©rios objetivos.

### Tarefa 5.1 â€” Rubrica de AvaliaÃ§Ã£o JurÃ­dica

- [ ] Criar `src/quality_rubric.py` com critÃ©rios mensurÃ¡veis:
  ```python
  RUBRICA = {
      "formato_correto": {
          "descricao": "Minuta segue seÃ§Ãµes I, II, III conforme modelo",
          "peso": 3,
          "check": "regex para I â€“, II â€“, III â€“"
      },
      "sem_alucinacao": {
          "descricao": "NÃ£o contÃ©m informaÃ§Ãµes nÃ£o presentes nas Etapas 1-2",
          "peso": 5,
          "check": "comparar dispositivos citados com Etapa 1"
      },
      "sumulas_corretas": {
          "descricao": "SÃºmulas da SeÃ§Ã£o III coincidem com Etapa 2",
          "peso": 4,
          "check": "extrair sÃºmulas de ambas e comparar"
      },
      "campos_preenchidos": {
          "descricao": "Nenhum placeholder [NÃƒO CONSTA] na minuta final",
          "peso": 2,
          "check": "buscar padrÃ£o [NÃƒO CONSTA] na saÃ­da"
      },
      "decisao_coerente": {
          "descricao": "DecisÃ£o (admitir/inadmitir) coerente com Ã³bices encontrados",
          "peso": 5,
          "check": "lÃ³gica: Ã³bices em todos temas â†’ inadmitir"
      }
  }
  ```

### Tarefa 5.2 â€” Pipeline de Auto-AvaliaÃ§Ã£o

- [ ] FunÃ§Ã£o `avaliar_minuta(resultado_etapa3, resultado_etapa1, resultado_etapa2) -> dict`:
  - Retorna score 0-100 com detalhes por critÃ©rio
  - Threshold mÃ­nimo configurÃ¡vel: `QUALITY_MIN_SCORE=70`
- [ ] Integrar na pipeline (apÃ³s Etapa 3):
  - Se score < threshold: logar alerta + marcar nos metadata
  - Se score >= threshold: normalidade
- [ ] Incluir no relatÃ³rio de auditoria

### Tarefa 5.3 â€” ComparaÃ§Ã£o com Minuta Gold

- [ ] Se uma minuta de referÃªncia foi usada, comparar:
  - Estrutura da saÃ­da vs. estrutura da referÃªncia
  - Percentual de aderÃªncia ao formato
  - Desvios significativos (adicionar ao alerta)

---

## Sprint 6 â€” Dashboard de Qualidade e MÃ©tricas

> **Meta:** Visibilidade contÃ­nua sobre a qualidade das minutas geradas.

### Tarefa 6.1 â€” MÃ©tricas de Treinamento

- [ ] Criar `src/training_metrics.py`:
  - Taxa de aprovaÃ§Ã£o por assessor (%)
  - Score mÃ©dio da auto-avaliaÃ§Ã£o por perÃ­odo
  - Minutas gold adicionadas vs. total gerado
  - Top 5 motivos de reprovaÃ§Ã£o (do feedback)
  - Custo mÃ©dio por anÃ¡lise (tokens Ã— preÃ§o)

### Tarefa 6.2 â€” Endpoint de Dashboard

- [ ] Criar `GET /dashboard` no `web_app.py`:
  - Cards com mÃ©tricas resumidas
  - GrÃ¡fico simples de evoluÃ§Ã£o de qualidade
  - Lista das Ãºltimas minutas com avaliaÃ§Ã£o
- [ ] Acessar dados de `outputs/feedback/` e metadata das anÃ¡lises

### Tarefa 6.3 â€” Alertas de RegressÃ£o

- [ ] Se taxa de aprovaÃ§Ã£o cair abaixo de 80% na Ãºltima semana: alerta
- [ ] Se score auto-avaliaÃ§Ã£o mÃ©dio cair mais de 10 pontos: alerta
- [ ] Integrar com `src/regression_alerts.py` (jÃ¡ existente)

---

## Sprint 7 â€” Refinamento do Prompt por Dados

> **Meta:** Usar o feedback acumulado para refinar automaticamente o SYSTEM_PROMPT.

### Tarefa 7.1 â€” AnÃ¡lise de PadrÃµes de Erro

- [ ] Script `scripts/analisar_erros.py`:
  - Ler todos os feedbacks de `outputs/feedback/`
  - Categorizar erros mais frequentes (alucinaÃ§Ã£o, formato, sÃºmula errada, etc.)
  - Gerar relatÃ³rio de recomendaÃ§Ãµes para ajuste do prompt

### Tarefa 7.2 â€” Regras de Anti-AlucinaÃ§Ã£o EspecÃ­ficas

- [ ] Baseado nos erros encontrados na Tarefa 7.1:
  - Adicionar regras especÃ­ficas no SYSTEM_PROMPT.md
  - Exemplos: "Nunca cite SÃºmula X quando o tema for Y"
  - Documentar em `prompts/PROMPT_CHANGELOG.md`

### Tarefa 7.3 â€” Testes de RegressÃ£o do Prompt

- [ ] Sempre que alterar o SYSTEM_PROMPT:
  1. Rodar pipeline contra as N minutas gold (golden_baseline.py)
  2. Comparar scores com a baseline anterior
  3. SÃ³ fazer deploy se score >= baseline
- [ ] Automatizar via script `scripts/test_prompt_regression.py`

---

## Sprint 8 â€” OtimizaÃ§Ãµes de Custo e Velocidade

> **Meta:** Reduzir custo por anÃ¡lise mantendo qualidade.

### Tarefa 8.1 â€” Cache SemÃ¢ntico de Temas Recorrentes

- [ ] Ativar `ENABLE_CACHING=true` no `.env`
- [ ] Configurar `src/cache_manager.py` para cachear:
  - Respostas de Etapa 2 quando tema/acÃ³rdÃ£o sÃ£o idÃªnticos
  - Trechos de transcriÃ§Ã£o reutilizÃ¡veis
- [ ] Medir economia real de tokens com cache ativo

### Tarefa 8.2 â€” Modelo HÃ­brido Otimizado

- [ ] Testar configuraÃ§Ã£o:
  - **ClassificaÃ§Ã£o + Chunks**: modelo leve (Qwen3 30B ou 72B, ~$0.01/M)
  - **Etapa 2 e 3 (anÃ¡lise jurÃ­dica)**: modelo parrudo (Qwen3 235B, $0.07/$0.46)
- [ ] Medir: custo real, qualidade (score rubrica), tempo
- [ ] Documentar comparaÃ§Ã£o em `docs/benchmark_modelos.md`

### Tarefa 8.3 â€” Processamento Paralelo da Etapa 2

- [ ] Ativar `ENABLE_PARALLEL_ETAPA2=true`
- [ ] Testar: temas analisados em paralelo (mÃºltiplos chunks simultÃ¢neos)
- [ ] Medir reduÃ§Ã£o de tempo total da pipeline

---

## Resumo de Entregas por Sprint

| Sprint | Entregas | Prioridade |
|--------|----------|------------|
| **1** | Base de minutas gold + formato + importador | ğŸ”´ CrÃ­tica |
| **2** | Seletor de similaridade (RAG leve) | ğŸ”´ CrÃ­tica |
| **3** | InjeÃ§Ã£o no prompt Etapa 3 (few-shot) | ğŸ”´ CrÃ­tica |
| **4** | UI de feedback + loop de treinamento | ğŸŸ¡ Alta |
| **5** | Auto-avaliaÃ§Ã£o por rubrica | ğŸŸ¡ Alta |
| **6** | Dashboard de qualidade | ğŸŸ¢ MÃ©dia |
| **7** | Refinamento de prompt por dados | ğŸŸ¢ MÃ©dia |
| **8** | OtimizaÃ§Ãµes de custo e velocidade | ğŸŸ¢ MÃ©dia |

---

## DependÃªncias e PrÃ©-Requisitos

- **Sprint 1** â†’ pode ser iniciada imediatamente
- **Sprint 2** â†’ depende da Sprint 1 (precisa das minutas gold)
- **Sprint 3** â†’ depende da Sprint 2 (precisa do seletor)
- **Sprint 4** â†’ pode ser iniciada em paralelo com Sprint 2-3
- **Sprint 5** â†’ pode ser iniciada em paralelo com Sprint 3-4
- **Sprint 6** â†’ depende de Sprint 4 e 5 (precisa de dados de feedback)
- **Sprint 7** â†’ depende de Sprint 6 (precisa de dados acumulados)
- **Sprint 8** â†’ pode ser iniciada a qualquer momento

---

## MÃ©tricas de Sucesso (KPIs)

| MÃ©trica | Baseline Atual | Meta Sprint 3 | Meta Sprint 6 |
|---------|---------------|---------------|---------------|
| Taxa de aprovaÃ§Ã£o do assessor | ~0% (nenhuma prestou) | 60% | 85% |
| Score auto-avaliaÃ§Ã£o | N/A | 70/100 | 85/100 |
| Custo por anÃ¡lise | ~$0.10 | ~$0.01 | ~$0.01 |
| Tempo por anÃ¡lise | ~3min | ~2min | ~1.5min |
| Minutas gold na base | 0 | 15 | 50+ |
| AlucinaÃ§Ãµes detectadas | Alta | <10% | <2% |
