# ==========================================
# ASSESSOR.AI - CONFIGURAÇÃO
# ==========================================

# OpenAI API (Obrigatória)
OPENAI_API_KEY=sk-proj-...

# LLM Provider (openai | openrouter | google)
# Default: openai
LLM_PROVIDER=openai

# ==========================================
# MODELOS DE IA
# ==========================================

# Modelo principal para análise jurídica
# GPT-4.1: Contexto 1M tokens | $2.00/M input, $8.00/M output
OPENAI_MODEL=gpt-4.1

# Temperatura (0.0 = determinístico, 1.0 = criativo)
TEMPERATURE=0.0

# Tokens máximos por resposta
MAX_TOKENS=2048

# ==========================================
# CONFIGURAÇÃO DE CHAMADAS LLM
# ==========================================

# Timeout de requisição (segundos)
LLM_TIMEOUT=120

# Máximo de tentativas em caso de erro
LLM_MAX_RETRIES=3

# ==========================================
# MODELOS HÍBRIDOS (ECONOMIA DE CUSTOS)
# ==========================================

# Modelo para classificação de documentos
# GPT-4.1-mini: 93% mais econômico | $0.15/M input, $0.60/M output
MODEL_CLASSIFICATION=gpt-4.1-mini

# Modelo para análise jurídica (Etapas 1-2)
MODEL_LEGAL_ANALYSIS=gpt-4.1

# Modelo para geração de minuta (Etapa 3)
MODEL_DRAFT_GENERATION=gpt-4.1

# ==========================================
# FEATURE FLAGS
# ==========================================

# Chunking inteligente para documentos grandes (recomendado: true)
ENABLE_CHUNKING=true

# Estratégia híbrida de modelos - reduz custos 60-80% (recomendado: true)
ENABLE_HYBRID_MODELS=true

# Gestão proativa de rate limit - previne erros 429 (recomendado: true)
ENABLE_RATE_LIMITING=true

# Cache de respostas LLM (recomendado: false inicialmente)
ENABLE_CACHING=false

# Processamento paralelo na Etapa 2 - 30% mais rápido (recomendado: false inicialmente)
ENABLE_PARALLEL_ETAPA2=false

# ==========================================
# GERENCIAMENTO DE TOKENS
# ==========================================

# Ratio do contexto a usar como budget (0.7 = 70% para margem de segurança)
TOKEN_BUDGET_RATIO=0.7

# Overlap de tokens entre chunks (para continuidade de contexto)
CHUNK_OVERLAP_TOKENS=500

# Máximo de tokens por requisição (respeita TPM de 30k/min do GPT-4.1)
MAX_CONTEXT_TOKENS=25000

# ==========================================
# CACHE E PARALELIZAÇÃO
# ==========================================

# TTL do cache em horas (respostas expiram após este tempo)
CACHE_TTL_HOURS=24

# Workers paralelos para processamento de temas na Etapa 2
# Manter baixo (2-3) para respeitar rate limits
ETAPA2_PARALLEL_WORKERS=3

# ==========================================
# LOGGING
# ==========================================

# Nível de log (DEBUG | INFO | WARNING | ERROR)
LOG_LEVEL=INFO

# ==========================================
# PROVIDERS ALTERNATIVOS (OPCIONAIS)
# ==========================================

# OpenRouter API (se LLM_PROVIDER=openrouter)
# OPENROUTER_API_KEY=sk-or-...
# OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# Google AI Studio (se LLM_PROVIDER=google)
# GOOGLE_API_KEY=...
