# OpenAI API
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o
MAX_TOKENS=2048
TEMPERATURE=0.0

# LLM call settings
LLM_TIMEOUT=120
LLM_MAX_RETRIES=3

# Logging
LOG_LEVEL=INFO

# ==========================================
# ROBUST ARCHITECTURE SETTINGS
# ==========================================

# Token Management
# Ratio of context limit to use as budget (0.7 = 70% for safety margin)
TOKEN_BUDGET_RATIO=0.7

# Token overlap between chunks for context continuity
CHUNK_OVERLAP_TOKENS=500

# Maximum context tokens per request (leaves buffer for response)
MAX_CONTEXT_TOKENS=100000

# Feature Flags
# Enable intelligent chunking for large documents (recommended: true)
ENABLE_CHUNKING=true

# Enable hybrid model strategy (gpt-4o-mini for simple tasks, gpt-4o for critical analysis)
# Reduces costs by 60-80% (recommended: true)
ENABLE_HYBRID_MODELS=true

# Enable proactive rate limit management (prevents 429 errors)
# Recommended: true
ENABLE_RATE_LIMITING=true

# Enable response caching (speeds up reruns by 50%)
# Recommended: false initially, enable after testing
ENABLE_CACHING=false

# Enable parallel processing in Stage 2 (30% faster)
# Recommended: false initially, enable after testing
ENABLE_PARALLEL_ETAPA2=false

# Hybrid Model Configuration
# Model used for classification (use mini for cost savings)
MODEL_CLASSIFICATION=gpt-4o-mini

# Model used for legal analysis (use gpt-4o for accuracy)
MODEL_LEGAL_ANALYSIS=gpt-4o

# Model used for draft generation (use gpt-4o for quality)
MODEL_DRAFT_GENERATION=gpt-4o

# Cache Configuration
# Cache TTL in hours (responses expire after this time)
CACHE_TTL_HOURS=24

# Parallel Processing
# Number of parallel workers for Stage 2 theme processing
# Keep low (2-3) to respect rate limits
ETAPA2_PARALLEL_WORKERS=3
